{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                                                                                                                \n",
    "                               ######################################################\n",
    "                               # ################################################## # \n",
    "                               # # write RL completely with numpy (no ML library) # #\n",
    "                               # #  (start)_ _ %             [find                # #\n",
    "                               # #         _ % _ (end)       shortest path        # #\n",
    "                               # #         _ _ _             from start to end]   # #\n",
    "                               # # % blocks, moves - E, W, N, S, NE, NW, SE, SW   # #\n",
    "                               # ################################################## #\n",
    "                               ######################################################\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statevalue(state):\n",
    "    statedict = {0 : ( 0, 0),\n",
    "                 1 : ( 0, 1),\n",
    "                 2 : ( 1, 0),\n",
    "                 3 : ( 2, 0),\n",
    "                 4 : ( 2, 1),\n",
    "                 5 : ( 2, 2),\n",
    "                 6 : ( 1, 2),\n",
    "                 7 : ( 0, 2),\n",
    "                 8 : ( 1, 1),\n",
    "                 9 : (-1,-1),\n",
    "                 10: (-1, 0),\n",
    "                 11: (-1, 1),\n",
    "                 12: (-1, 2),\n",
    "                 13: (-1, 3),\n",
    "                 14: ( 0, 3),\n",
    "                 15: ( 1, 3),\n",
    "                 16: ( 2, 3),\n",
    "                 17: ( 3, 3),\n",
    "                 18: ( 3, 2),\n",
    "                 19: ( 3, 1),\n",
    "                 20: ( 3, 0),\n",
    "                 21: ( 3,-1),\n",
    "                 22: ( 2,-1),\n",
    "                 23: ( 1,-1),\n",
    "                 24: ( 0,-1)}\n",
    "    return np.array(statedict[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextstate(s, move):\n",
    "    statedict = {0 : ( 0, 0),\n",
    "                 1 : ( 0, 1),\n",
    "                 2 : ( 1, 0),\n",
    "                 3 : ( 2, 0),\n",
    "                 4 : ( 2, 1),\n",
    "                 5 : ( 2, 2),\n",
    "                 6 : ( 1, 2),\n",
    "                 7 : ( 0, 2),\n",
    "                 8 : ( 1, 1),\n",
    "                 9 : (-1,-1),\n",
    "                 10: (-1, 0),\n",
    "                 11: (-1, 1),\n",
    "                 12: (-1, 2),\n",
    "                 13: (-1, 3),\n",
    "                 14: ( 0, 3),\n",
    "                 15: ( 1, 3),\n",
    "                 16: ( 2, 3),\n",
    "                 17: ( 3, 3),\n",
    "                 18: ( 3, 2),\n",
    "                 19: ( 3, 1),\n",
    "                 20: ( 3, 0),\n",
    "                 21: ( 3,-1),\n",
    "                 22: ( 2,-1),\n",
    "                 23: ( 1,-1),\n",
    "                 24: ( 0,-1)}\n",
    "    s = statevalue(s)\n",
    "    snext = s + movevalue(move)\n",
    "    for _ in statedict.keys():\n",
    "        if ( np.array_equal(np.array(statedict[_]), snext)):\n",
    "            return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movevalue(move):\n",
    "    movedict = {\"N\" : (-1, 0), \n",
    "                \"S\" : ( 1, 0), \n",
    "                \"E\" : ( 0, 1), \n",
    "                \"W\" : ( 0,-1), \n",
    "                \"NE\": (-1, 1), \n",
    "                \"SE\": ( 1, 1),  \n",
    "                \"NW\": (-1,-1), \n",
    "                \"SW\": (-1, 1)}\n",
    "    return np.array(movedict[move])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(s,move):\n",
    "    snext = nextstate(s, move)\n",
    "    ##cartesian distance between the cell-centers:\n",
    "    d = math.sqrt(sum(list(statevalue(snext)**2 + statevalue(s)**2)))\n",
    "    \n",
    "    epstat = episodestatus(statevalue(snext))\n",
    "    if (epstat == \"s\"): d = 1000/d ##reward on complete the journey\n",
    "    if (epstat == \"f\"): d = -1/d ##penalize for falling out of the universe or bumping into an obstacle\n",
    "    if (epstat == \"t\"): \n",
    "        if (snext == 0): d = 0 ##don't come back to the start position\n",
    "        else: d = 1/d ##keep moving\n",
    "    return d, snext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodestatus(s):\n",
    "    ##fall out of universe\n",
    "    edge = (len([_ for _ in s if (_ > -1) and (_ < 3)]) < 2)\n",
    "    ##hit an obstacle\n",
    "    o1 = statevalue(obstacle1) ## obstacle1\n",
    "    o2 = statevalue(obstacle2) ## obstacle2\n",
    "    hitobstacle = (np.array_equal(s, o1) or np.array_equal(s, o2))\n",
    "    ##made it to destination:\n",
    "    destination = statevalue(final)\n",
    "    success = np.array_equal(s, destination)\n",
    "    if success : return \"s\" ##successfully completed the journey\n",
    "    elif (hitobstacle or edge) : return \"f\" ##failed to reached the destination\n",
    "    else: return \"t\" ##keep moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlearn():\n",
    "    ##completes and episode\n",
    "    s = start\n",
    "    qnewMatrix = np.copy(Qmatrix)\n",
    "    while (episodestatus(statevalue(s)) == \"t\"):\n",
    "        movekey = np.random.randint(0,7)\n",
    "        r, snext = reward(s,moves[movekey])\n",
    "        deltaq = r + (gamma*(np.amax(qnewMatrix[snext])) if (episodestatus(statevalue(snext))==\"t\") else 0)\n",
    "        qnewMatrix[s][movekey] = qnewMatrix[s][movekey] + alpha*(deltaq - qnewMatrix[s][movekey])   \n",
    "        s = snext\n",
    "    return qnewMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimumPolicy():\n",
    "    s = start\n",
    "    path = [statevalue(s)]\n",
    "    while (episodestatus(statevalue(s)) == \"t\"):\n",
    "        moveindex = np.argmax(Qmatrix[s])\n",
    "        move = moves[moveindex]\n",
    "        print(move)\n",
    "        s = nextstate(s, move)\n",
    "        path.append(statevalue(s))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episodes run:  972\n",
      "E\n",
      "SE\n",
      "optimal policy:  [array([0, 0]), array([0, 1]), array([1, 2])]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "      this program is to find the shortest path from \n",
    "      start to end cell for a simple 3x3 grid with two obstacles\n",
    "      with Q-learning (written only using numpy functions):\n",
    "      (start)_ _ %\n",
    "             _ % _ (end)\n",
    "             _ _ _      \n",
    "       *(%) are the two obstacles\n",
    "       *cell coordinate (x,y):\n",
    "           x : row number\n",
    "           y : column number\n",
    "       *moves allowed:\n",
    "           usual 8: cardinal and intercardinal directions\n",
    "    \"\"\"\n",
    "\n",
    "    states = [0, 1, 2, 3, 4, 5] ##universe (excluding the final cell)\n",
    "    start = 0 ##start cell\n",
    "    final = 6 ##end cell\n",
    "    ##obstacles:\n",
    "    obstacle1 = 7\n",
    "    obstacle2 = 8\n",
    "    \n",
    "    ##cardinal and intercardinal directions to move:\n",
    "    moves  = {0:\"N\", 1:\"S\", 2:\"E\", 3:\"W\", 4:\"NE\", 5:\"SE\", 6:\"NW\", 7:\"SW\"}\n",
    "    \n",
    "    ##hyperparameters:\n",
    "    gamma = 0.5 ##discount factor\n",
    "    alpha = 0.2 ##learning rate\n",
    "    \n",
    "    ##initializations:\n",
    "    Qmatrix = np.zeros([len(states), len(moves)])  ##initialized Q-matrix\n",
    "    episodes = 0\n",
    "    ##do Q-learining:\n",
    "    while (episodes < 2000):\n",
    "        q = Qlearn()\n",
    "        episodes += 1\n",
    "        #print(\"episode: \", episodes)\n",
    "        if (np.array_equal(q,Qmatrix)): \n",
    "            break    ##Qmatrix converged\n",
    "        Qmatrix = np.copy(q)\n",
    "    \n",
    "    ##Solution:\n",
    "    #print(Qmatrix)\n",
    "    print(\"episodes run: \", episodes)\n",
    "    print(\"optimal policy: \", OptimumPolicy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
